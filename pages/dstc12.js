import React, { Component } from 'react';
import fetch from 'isomorphic-unfetch';
import Header from '../components/Header';
import Footer from '../components/Footer';

const API_URL = process.env.API_URL;

const STYLE = {
  cell: {
    'text-align': "center",
    'vertical-align': "middle",
    'white-space': 'pre',
  },
  td: {
    'vertical-align': "middle"
  },
};

class DSTC12 extends Component {
  render() {
    return (
      <div>
        <Header/>
        <main role="main" className="container">
          <h1 className="mt-5 font-weight-bold">DSTC12: Dialogue System Technology Challenge 12</h1>
          <h2 className="mt-5 font-weight-bold">Track 1: Dialog System Evaluation: Dimensionality, Language, Culture and Safety</h2>
            <br/>
            <p> <b>(02/05)</b> Official Leaderboard released!</p>
            <p> <b>(19/04)</b> Task1 Codabench website and test data now online. Click <a href="https://www.codabench.org/competitions/7579/">here</a> for access!</p>
            <p> <b>(19/04)</b> Task2 Codabench website and test data now online. Click <a href="https://www.codabench.org/competitions/7204/">here</a> for access!</p>
            <p> <b>(19/03)</b> Task Schedule has been updated to reflect data release delays. Submission deadline is now <span style={{color: 'green'}}> Apr 28 (23:59 Anywhere on Earth (AoE), UTC-12)</span>.</p>
            <p> <b>(10/03)</b> Task1 development data and baseline results are <span style={{color: 'green'}}> now available.</span></p>
            <p> <b>(03/01)</b> Task2 development data and baseline results are <span style={{color: 'green'}}> now available.</span></p>
            <p> <b>(19/12)</b> Click <a href="https://forms.gle/J8GrZchnFEacJ7Y47">here</a> to register for DSTC12.T1.</p>
            <hr/>
          
          <h3 className="font-weight-bold">Track Overview</h3>
            <br/>
            <p style={{marginLeft: 60 + 'px'}} align='justify'>
            For this track, we propose two evaluation tasks for open-domain dialogue systems:
              <br/><br/>
              <ol>
                <li><b>Dialogue-level and Multi-dimensional Automatic Evaluation Metrics.</b> Participants in this task are expected to design automatic metrics that evaluate conversations at the dialogue level (not solely turn-level) and over multiple evaluation dimensions.</li>
                <li><b>Multilingual and Multicultural Safety Detection.</b> Participants in this task are expected to develop safety classifiers that detect whether a response is unsafe.</li>
              </ol>

            <img src="./static/img/warning.png" alt="Logo_EC" style={{width:  + 'px', height: 35 + 'px'}}/> &nbsp; For both tasks, participants are constrained to use open-access LMs and LLMs of size lower than 13B paramters. 
            </p>
            <hr/>

            <h3 className="font-weight-bold">Task 1: Dialogue-level and Multi-dimensional Automatic Evaluation Metrics</h3>
            <br/><br/>
            <ul>
              <li><b>Overview</b></li>
            </ul>
            <p style={{marginLeft: 60 + 'px'}} align='justify'>
            Previous challenges and works focus more on turn-level dialogue evaluation (Zhang et al., 2022; Rodríguez-Cantelar et al., 2023; Yeh et al., 2021) and lack further investigation of dialogue-level evaluation through automatic metrics. As LLMs advance, aspects of conversations beyond coherence, fluency, etc. should also be studied. Addiitonally, these aspects should provide a more fine-grained analysis of the levels of quality for the whole conversation.
            </p>

            <ul>
              <li><b>Goals</b></li>
            </ul>
            <p style={{marginLeft: 60 + 'px'}} align='justify'>
            Participants will develop automatic evaluation metrics for open-domain dialogue. The system should be able to evaluate up to 10 different dimensions including previous common ones (i.e. coherence, engageness, or naturalness), together with new ones like empathy and error handling in this challenge.
            </p>

            <ul>
              <li><b>Evaluation</b></li>
            </ul>
            <p style={{marginLeft: 60 + 'px'}} align='justify'>
            Based on the dialogue-level evaluation scores generated by the proposed evaluation metric for 10 selected dimensions on the testing dataset, we will calculate Spearman’s correlation between human annotations (from MTurk workers or lab members) and metric-generated scores among dimensions. An averaged correlation coefficient is calculated to rank the automatic evaluation metrics submitted in the end.
            </p>

            <ul>
              <li id="provided-datasets"><b>Provided Datasets</b></li>
            </ul>
            <p style={{marginLeft: 60 + 'px'}} align='justify'>
            <img src="./static/img/download.png" alt="Logo_EC" style={{width:  + 'px', height: 35 + 'px'}}/> &nbsp; 
            Task 1 development data is available for download <a href="https://huggingface.co/datasets/codesj/dstc12_track1_chateval">here</a>. Note that the datasets are only available for registered participants.
            </p>

            <ul>
              <li id="baselines"><b>Baseline</b></li>
            </ul>
            <p style={{marginLeft: 60 + 'px'}} align='justify'>
            As a baseline, <a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct">Llama-3.1-8B-Instruct</a> is used. This model is a post-trained version of LLama-3.1-8B using supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. We prompt the model with a dialogue-evaluation instruction.

            <br/><br/>

            We report the Pearson score for the Llama-Guard-3-1B model on the development subset made available for this task. The results are as follows:

            <font size="2" face="Courier New" >
            <table width="50%" className="table table-bordered">
              <thead>
                <tr>
                  <th scope="col">Dimension</th>
                  <th scope="col">Pearson</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Empathy</td>
                  <td>0.0829</td>
                </tr>
                <tr>
                  <td>Trust</td>
                  <td>0.2546</td>
                </tr>
                <tr>
                  <td>Skill</td>
                  <td>0.2689</td>
                </tr>
                <tr>
                  <td>Talent</td>
                  <td>0.144</td>
                </tr>
                <tr>
                  <td>Capability</td>
                  <td>0.2467</td>
                </tr>
                <tr>
                  <td>Relevance</td>
                  <td>-0.1319</td>
                </tr>
                <tr>
                  <td>Non-Repetition</td>
                  <td>-0.1883</td>
                </tr>
                <tr>
                  <td>Proactivity</td>
                  <td>-0.5387</td>
                </tr>
                <tr>
                  <td>Curiosity</td>
                  <td>-0.3892</td>
                </tr>
                <tr>
                  <td>Overall</td>
                  <td>0.4234</td>
                </tr>
                <tr>
                  <td>Average</td>
                  <td>0.01724</td>
                </tr>
                <tr>
                  <td><b>Average (absolute)</b></td>
                  <td>0.26686</td>
                </tr>
              </tbody>
            </table>
            </font>

            <br/><br/>

            A minimalistic reproducible code for the baseline based on VLLM is available <a href="https://huggingface.co/datasets/codesj/dstc12_track1_chateval/blob/main/LLama_task1.py">here</a>.

            
            </p>
            
            <ul>
              <li><b>Leaderboard</b></li>
            </ul>
            
            <font size="2" face="Courier New" >
            <table width="50%" className="table table-bordered">
              <tbody>
                <tr>
                  <th>Submission Rank</th>
                  <th>Team</th>
                  <th>Codabench ID</th>
                  <th>Average (absolute)</th>
                  <th>Average</th>
                  <th>Empathy</th>
                  <th>Trust</th>
                  <th>Skill</th>
                  <th>Talent</th>
                  <th>Capability</th>
                  <th>Relevance</th>
                  <th>Non-repetition</th>
                  <th>Proactivity</th>
                  <th>Curiosity</th>
                  <th>Overall</th>
                  </tr>
                  <tr>
                    <td bgcolor="#4285f4">1</td><td bgcolor="#4285f4">baseline</td><td></td><td bgcolor="#4285f4">0.1681</td><td>0.1217</td><td>0.0647</td><td>-0.1117</td><td>-0.0955</td><td>0.0962</td><td>0.0677</td><td>0.2337</td><td>0.3851</td><td>-0.0248</td><td>0.2253</td><td>0.3766</td></tr><tr>
                    <td bgcolor="#34a853">2</td><td bgcolor="#34a853">ORALIS</td><td>278269</td><td bgcolor="#34a853">0.1503</td><td>0.0605</td><td>-0.0788</td><td>0.0067</td><td>-0.2246</td><td>0.0529</td><td>0.1276</td><td>0.0808</td><td>0.1071</td><td>-0.1457</td><td>0.3687</td><td>0.3099</td></tr><tr>
                    <td bgcolor="#ff6d01">3</td><td>ORALIS</td><td>278268</td><td bgcolor="#ff6d01">0.1471</td><td >0.1265</td><td>0.1720</td><td>0.2004</td><td>0.0660</td><td>0.2409</td><td>0.2439</td><td>-0.1032</td><td>0.1432</td><td>0.0821</td><td>0.0913</td><td>0.1283</td></tr><tr>
                    <td>4</td><td>ORALIS</td><td>727820</td><td>0.1408</td><td>0.0595</td><td>-0.0788</td><td>0.0067</td><td>-0.2246</td><td>0.2409</td><td>0.1276</td><td>-0.1032</td><td>0.1432</td><td>0.0821</td><td>0.0913</td><td>0.3099</td></tr><tr>
                    <td>5</td><td>ORALIS</td><td>278271</td><td>0.1374</td><td>0.0105</td><td>-0.1658</td><td>0.1254</td><td>-0.0183</td><td>0.2240</td><td>0.1157</td><td>-0.2831</td><td>-0.0018</td><td>0.1962</td><td>0.0784</td><td>-0.1652</td></tr><tr>
                    <td>6</td><td bgcolor="#ff6d01">qqprun</td><td>278272</td><td>0.1360</td><td>0.0904</td><td>-0.1183</td><td>0.2407</td><td>-0.1094</td><td>0.0873</td><td>0.1717</td><td>0.0738</td><td>0.2220</td><td>0.0185</td><td>0.0064</td><td>0.3117</td></tr><tr>
                    <td>7</td><td>qqprun</td><td>278273</td><td>0.0730</td><td>0.0649</td><td>-0.0035</td><td>0.0594</td><td>0.0046</td><td>0.0948</td><td>0.1101</td><td>0.1439</td><td>-0.0371</td><td>0.0758</td><td>0.0221</td><td>0.1786</td></tr>
                  </tbody>
              </table>
              </font>

            <hr/>

            <h3 className="font-weight-bold">Task 2: Multilingual and Multicultural Safety Detection</h3>
            <br/><br/>
            <ul>
              <li><b>Overview</b></li>
            </ul>
            <p style={{marginLeft: 60 + 'px'}} align='justify'>
            Users are increasingly challenging current LLMs to generate harmful and/or unsafe answers. In addition, even without adversarial probing, generated responses may contain unhelpful and/or harmful content. Therefore, the automatic detection of this content is important in the deployment of these systems. Unfortunately, safety evaluation frameworks frequently narrow the notion of safety to strict definitions of bias and toxicity (Shuster et al., 2022; Ouyang et al., 2022), discarding other safety aspects. This task expands on earlier safety detection tasks, introducing other risk aspects such as unqualified and harmful advice, manipulation, and illegal activities.
            <br/><br/>
            Safety considerations in prior and current generation chatbots are limited to North American notions of safety and harm. In Task 2, we will expand safety datasets to a diverse set of languages and cultures, with human annotations performed by representatives of said cultures. Beyond facilitating the study of safety across cultures, it also allows for the evaluation of the robustness of safety classifiers in terms of culture and language. In this Task, we will target at least 4 different languages in the test set: English, Chinese, Portuguese, and Spanish. Additional languages may be added (within the language set provided for development).
            </p>

            <ul>
              <li><b>Goals</b></li>
            </ul>
            <p style={{marginLeft: 60 + 'px'}} align='justify'>
            Participants will develop automatic safety classifiers of responses generated by LLMs across different languages and cultures. The safety detectors should be able to generalize across different languages and cultures.
            </p>

            <ul>
              <li><b>Evaluation</b></li>
            </ul>
            <p style={{marginLeft: 60 + 'px'}} align='justify'>
            For the overall submission rankings, we will use the ROC-AUC score to evaluate the performance of the safety detectors developed by the participants on a multilingual hidden test set. The ROC-AUC score is computed at the response level. Additionally, for a more fine-grained analysis of the participants’ performance, we report their language/culture-wise ROC-AUC score.
            </p>
            <ul>
              <li id="provided-datasets"><b>Provided Datasets</b></li>
            </ul>
            <p style={{marginLeft: 60 + 'px'}} align='justify'>
            For Task 2, we have identified and processed three datasets for development: 
            <br/><br/>
            <li><b>Bot Adversarial Dialogue</b> (Xu et al., NAACL 2021)</li>
            <li><b>Dialogue Safety</b> (Dinan et al., EMNLP-IJCNLP 2019)</li>
            <li><b>ProsocialDialog</b> (Kim et al., EMNLP 2022)</li>
            
            <br/><br/>
            These datasets were selected for their conversational nature and existing safety annotations. However, participants are free to use any other <b>open-access</b> resources during the development of their systems. 
            <br/><br/>
            All datasets contain train/val/test splits. We automatically translate these datasets in to several target languages (Arabic, German, Spanish, French, Japanese, Portuguese and Chinese) using <tt>gpt-4o-mini</tt>, for which we report an average translation quality score of 0.7153 (using <tt>wmt23-cometkiwi-da-xl</tt>). 
            <br/><br/>
            <img src="./static/img/download.png" alt="Logo_EC" style={{width:  + 'px', height: 35 + 'px'}}/> &nbsp; Datasets are avilable to download <a href="https://huggingface.co/collections/dstc12/task2-6777ebf32979bfe4f9c41f3d">here</a>. Note that the datasets are only available for registered participants.
            
            </p>

            <ul>
              <li id="provided-baseline"><b>Baseline</b></li>
            </ul>
            <p style={{marginLeft: 60 + 'px'}} align='justify'>
            As a baseline, <a href="https://huggingface.co/meta-llama/Llama-Guard-3-1B">Llama-Guard-3-1B</a> will be used. This model is a fine-tuned Llama-3.2-1B pretrained model for content safety classification. We report baseline results below.
            
            <br/><br/>

            We report the ROC-AUC score for the Llama-Guard-3-1B model on the development subsets made available for this task. The results are as follows:
            
            <font size="2" face="Courier New" >
            <table width="50%" className="table table-bordered">
              <thead>
                <tr>
                  <th scope="col">Language</th>
                  <th scope="col">ROC-AUC</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Arabic (ar)</td>
                  <td>0.6432</td>
                </tr>
                <tr>
                  <td>German (de)</td>
                  <td>0.6616</td>
                </tr>
                <tr>
                  <td>English (en)</td>
                  <td>0.7137</td>
                </tr>
                <tr>
                  <td>Spanish (es)</td>
                  <td>0.6557</td>
                </tr>
                <tr>
                  <td>French (fr)</td>
                  <td>0.6642</td>
                </tr>
                <tr>
                  <td>Japanese (ja)</td>
                  <td>0.6537</td>
                </tr>
                <tr>
                  <td>Portuguese (pt)</td>
                  <td>0.6687</td>
                </tr>
                <tr>
                  <td>Chinese (zh)</td>
                  <td>0.6765</td>
                </tr>
                <tr>
                  <td><b>Average (all)</b></td>
                  <td>0.6672</td>
                </tr>
              </tbody>
            </table>
            </font>
            </p>

            Per dataset results are available on each dataset page.

            <br/><br/>

            A minimalistic reproducible code for the baseline based on VLLM is available <a href="https://huggingface.co/datasets/dstc12/bot_adversarial_dialogue/blob/main/LlamaGuard.py">here</a>.

            <br/><br/>

            <ul>
              <li><b>Leaderboard</b></li>
            </ul>
            
            <font size="2" face="Courier New" >
            <table width="50%" className="table table-bordered">
            <tbody>
              <tr>
                <th>Submission Rank</th>
                <th>Team</th>
                <th>Codabench ID</th>
                <th>Average</th>
                <th>Cultural</th>
                <th>Multilingual</th>
                <th>AR</th>
                <th>DE</th>
                <th>EN</th>
                <th>ES</th>
                <th>FR</th>
                <th>JA</th>
                <th>PT</th>
                <th>ZH</th>
              </tr>
              <tr><td bgcolor="#4285f4">1</td><td bgcolor="#4285f4" >VAI-CORE</td><td>273546</td><td bgcolor="#4285f4">0.9046</td><td>0.4227</td><td bgcolor="#4285f4">0.9648</td><td>0.9623</td><td>0.9700</td><td>0.9753</td><td>0.9695</td><td>0.9697</td><td>0.9607</td><td>0.9455</td><td>0.9656</td></tr>
              <tr><td bgcolor="#34a853">2</td><td>VAI-CORE</td><td>277851</td><td bgcolor="#34a853">0.8885</td><td bgcolor="#34a853">0.4831</td><td bgcolor="#34a853">0.9392</td><td>0.9599</td><td>0.9723</td><td>0.9736</td><td>0.9685</td><td>0.9677</td><td>0.9531</td><td>0.7499</td><td>0.9684</td></tr>
              <tr><td bgcolor="#ff6d01">3</td><td bgcolor="#34a853">qqpprun</td><td>273763</td><td bgcolor="#ff6d01">0.8078</td><td>0.4571</td><td bgcolor="#ff6d01">0.8517</td><td>0.8492</td><td>0.8714</td><td>0.8781</td><td>0.8688</td><td>0.8563</td><td>0.8147</td><td>0.8691</td><td>0.8057</td></tr>
              <tr><td>4</td><td>qqpprun</td><td>273499</td><td>0.7780</td><td>0.3476</td><td>0.8318</td><td>0.8234</td><td>0.8505</td><td>0.8534</td><td>0.8364</td><td>0.8246</td><td>0.8023</td><td>0.8328</td><td>0.8311</td></tr>
              <tr><td>5</td><td bgcolor="#ff6d01">baseline</td><td></td><td>0.7767</td><td bgcolor="#4285f4">0.5126</td><td>0.8097</td><td>0.8056</td><td>0.8067</td><td>0.8179</td><td>0.8034</td><td>0.8178</td><td>0.7957</td><td>0.8162</td><td>0.8142</td></tr>
              <tr><td>6</td><td>qqpprun</td><td>273497</td><td>0.7468</td><td>0.4658</td><td>0.7819</td><td>0.7782</td><td>0.8015</td><td>0.7892</td><td>0.7941</td><td>0.7773</td><td>0.7557</td><td>0.7877</td><td>0.7716</td></tr>
              <tr><td>7</td><td>qqpprun</td><td>273824</td><td>0.6667</td><td bgcolor="#ff6d01">0.4830</td><td>0.6896</td><td>0.6917</td><td>0.7000</td><td>0.6854</td><td>0.6979</td><td>0.6888</td><td>0.6804</td><td>0.6843</td><td>0.6884</td></tr>
              </tbody>
              </table>
              </font>

            <hr/>
          <h3 className="font-weight-bold">Citation</h3>
            <br/>
            <p>If you are using Task data please use the following citation:</p>
            
                        <pre><code className="bibtex">
            {`@inproceedings{mendonca2025dstc12t1,
                author = "John Mendonça and Lining Zhang and Rahul Mallidi and Luis Fernando D'Haro and João Sedoc",
                title = "Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12",
                booktitle = "DSTC12: The Twelfth Dialog System Technology Challenge",
                series = "26th Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)",
                year = 2025,
                month = "September",
                address = "Avignon, France"
            }`}
                        </code></pre>


            <hr/>

          <h3 className="font-weight-bold">Registration Details</h3>
            <br/>
            <p>To become an official DSTC12 Track 1 participant, you must be registered using this <a href="https://forms.gle/J8GrZchnFEacJ7Y47"> Form</a>. Once registered, you will be able to download the datasets and readme documents as well as submit your results.</p>
            <p>There must be only one team per laboratory or research group. The members of the same team must be under a single registration, that is, the team leader must register his entire team by giving their e-mail addresses in addition to his own.</p>
            <p>Any updates and information about the tracks will be posted on the <a href="https://dstc12.dstc.community/">DSTC12 official website</a>, or check the <a href="https://groups.google.com/a/dstc.community/g/list">DSTC Mailing List</a>.</p>
            <hr/>
          <h3 className="font-weight-bold">Schedule</h3>
            <br/>
            <ul>
              <li><b>Test data release</b>: Apr 18</li>
              <li><b>Entry submission deadline</b>: Apr 28 (23:59 Anywhere on Earth (AoE), UTC-12)</li>
              <li><b>Final result announcement</b>: May 2</li>
              <li><b>Paper submission</b>: Jun 1</li>
              <li><b>Workshop (SIGDIAL) </b>: Aug 28</li>
            </ul>
            <hr/>
          <h3 className="font-weight-bold">Organizers</h3>
            <br/>
            <ul>
              <li>John Mendonça (INESC-ID/IST, Portugal) - <b>john.mendonca@inesc-id.pt</b></li>
              <li>Lining Zhang (New York University, USA)</li>
              <li>Alon Lavie (Carnegie Mellon University, USA)</li>
              <li>Isabel Trancoso (INESC-ID/IST, Portugal)</li>
              <li>Jo&atilde;o Sedoc (New York University, USA)</li>
              <li>Luis F. D'Haro (Universidad Polit&eacute;cnica de Madrid, Spain)</li>
            </ul>
            <hr/>
          <h3 className="font-weight-bold">Contact</h3>
            <br/>
            <p>For queries related to the challenge contact the organizers via the <a href="https://groups.google.com/a/dstc.community/g/list">DSTC Mailing List.</a></p>
            <hr/>
          <h3 className="font-weight-bold">Acknowledgement</h3>
            <br/>
            <p>This research was supported by the Portuguese Recovery and Resilience Plan through project C645008882-00000055 (Responsible.AI), by Portuguese national funds through Fundação para a Ciência e Tecnologia (FCT) with references PRT/BD/152198/2021 and DOI:10.54499/UIDB/50021/2020.</p>
            <p>This work is supported by the European Commission through Project ASTOUND (101071191 — HORIZON EIC-2021- PATHFINDERCHALLENGES-01), and by project BEWORD (PID2021-126061OB-C43) funded by MCIN/AEI/10.13039/501100011033 and, as appropriate, by “ERDF A way of making Europe”, by the “European Union”. </p>
            <p>We also want to give thanks to MS Azure services (especially to Irving Kwong) for their sponsorship to continue processing new datasets that could be interesting for the dialogue community.</p>
            <p>This research project is supported by the NYU ChatEval Team led by João Sedoc.</p>
            <p> <img src="./static/img/Logo_EC.png" alt="Logo_EC" style={{width: 375 + 'px', height: 75 + 'px'}}/> <img src="./static/img/Logo_PRR.png" alt="Logo_PRR" style={{width: 214 + 'px', height: 74 + 'px'}}/>
            </p>
            <hr/>
          <h3 className="font-weight-bold">FAQ</h3>
            <br/>
            <h5 className="card-title">How much does participate in this Track cost?</h5>
            <p className="card-text">This Track is currently <mark>free</mark> for everyone.</p>
            <hr/>

          <h3 className="font-weight-bold">References</h3>
            <br/>
            <ul>
              <li>Chen Zhang, João Sedoc, Luis Fernando D'Haro, Rafael Banchs, and Alexander Rudnicky. "Automatic evaluation and moderation of open-domain dialogue systems." arXiv preprint arXiv:2111.02110 (2021).</li>
              <li>Mario Rodríguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, João Sedoc, Luis Fernando D'Haro, and Alexander Rudnicky. "Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4." arXiv preprint arXiv:2306.12794 (2023).</li>
              <li>Chulaka Gunasekara, Seokhwan Kim, Luis Fernando D'Haro, Abhinav Rastogi, Yun-Nung Chen, Mihail Eric, Behnam Hedayatnia, et al. "Overview of the ninth dialog system technology challenge: Dstc9." arXiv preprint arXiv:2011.06486 (2020).</li>
              <li>Sarik Ghazarian, Behnam Hedayatnia, Alexandros Papangelis, Yang Liu, and Dilek Hakkani-Tur. 2022. What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation. In Findings of the Association for Computational Linguistics: ACL 2022, pages 4194–4204, Dublin, Ireland. Association for Computational Linguistics.</li>
              <li>Yi-Ting Yeh, Maxine Eskenazi, and Shikib Mehri. 2021. A Comprehensive Assessment of Dialog Evaluation Metrics. In The First Workshop on Evaluations and Assessments of Neural Conversation Systems, pages 15–33, Online. Association for Computational Linguistics.</li>
              <li>Jing Xu, Da Ju, Joshua Lane, Mojtaba Komeili, Eric Michael Smith, Megan Ung, Morteza Behrooz, et al. "Improving Open Language Models by Learning from Organic Interactions." arXiv preprint arXiv:2306.04707 (2023).</li>
              <li>Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, W.K.F. Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur and Jason Weston. “BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage.” ArXiv abs/2208.03188 (2022): n. Pag.</li>
              <li>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin et al. "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena." arXiv preprint arXiv:2306.05685 (2023).</li>
              <li>Jianguo Zhang, Kun Qian, Zhiwei Liu, Shelby Heinecke, Rui Meng, Ye Liu, Zhou Yu, Silvio Savarese, and Caiming Xiong. "DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI." arXiv preprint arXiv:2307.10172 (2023).</li>
              <li>Ekaterina Svikhnushina, Anastasiia Filippova, and Pearl Pu. 2022. iEval: Interactive Evaluation Framework for Open-Domain Empathetic Chatbots. In Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 419–431, Edinburgh, UK. Association for Computational Linguistics.</li>
              <li>Sarah E. Finch and Jinho D. Choi. 2020. Towards Unified Dialogue System Evaluation: A Comprehensive Analysis of Current Evaluation Protocols. In Proceedings of the 21st Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 236–245, 1st virtual meeting. Association for Computational Linguistics.</li>
              <li>Damilola Omitaomu, Shabnam Tafreshi, Tingting Liu, Sven Buechel, Chris Callison-Burch, Johannes C. Eichstaedt, Lyle Ungar and João Sedoc. “Empathic Conversations: A Multi-level Dataset of Contextualized Conversations.” ArXiv abs/2205.12698 (2022): n. pag.</li>
              <li>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, & Ryan Lowe. “Training language models to follow instructions with human feedback.”arXiv preprint arXiv:2203.02155 (2022). </li>
              <li>Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, & Jack Clark.. “Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned.” arXiv preprint arXiv:2204.05862 (2022).</li>
              <li>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, & Jared Kaplan. “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.” arXiv preprint arXiv:2204.05862 (2022).</li>
              <li>Emily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston. 2019. Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4537–4546, Hong Kong, China. Association for Computational Linguistics.</li>
              <li>Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. 2021. Bot-Adversarial Dialogue for Safe Conversational Agents. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2950–2968, Online. Association for Computational Linguistics.</li>
              <li>Hyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing Lu, Daniel Khashabi, Gunhee Kim, Yejin Choi, and Maarten Sap. 2022. ProsocialDialog: A Prosocial Backbone for Conversational Agents. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4005–4029, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</li>
            </ul>


          <p>&nbsp;</p>

        </main>
        <Footer />
      </div>
    )
  }
}

DSTC12.getInitialProps = async function() {

  const provided_datasets_summary = [
    {
      id: 'provided_datasets_summary',
      datasets: {
        name: "",
        description: ""
      },
      info: [
        {id: 'num_dataset', name: '#Datasets', chanel: '18', dstc10: '7', cdial: '3'},
        {id: 'language', name: 'Language', chanel: 'English, Spanish/Chinese,\nand English back-translation', dstc10: 'English, Spanish/Chinese,\nand English back-translation', cdial: 'Chinese, English,\nand Chinese back-translation'},
        {id: 'dialogues_type', name: 'Dialogues Type', chanel: 'Human-Human Open-Domain', dstc10: 'Human-Chatbot Open-Domain', cdial: 'Human-Human Open-Domain'},
        {id: 'num_dialogues_utterances', name: '# Dialogues/\nUtterances', chanel: '+ 390.000 / + 3.000.000', dstc10: '+ 18.000 / + 55.000', cdial: '+ 3.470 / +130.000'},
        {id: 'annotations', name: 'Annotations', chanel: 'Sentiment analysis and Toxicity', dstc10: 'Sentiment analysis and Toxicity\nTurn/dialogue level human scores', cdial: 'Turn/dialogue level human scores'},
        {id: 'task1_set', name: 'Task 1 Set', chanel: 'Public: Train', dstc10: 'Public: Dev, Test\nHidden: Automatic Translations', cdial: 'Public: Train/Dev/Test'},
        {id: 'task2_set', name: 'Task 2 Set', chanel: 'Public: Train', dstc10: 'Public: Dev, Test\nHidden: Manually paraphrased/back-translation', cdial: '—'},
      ]
    }
  ]

  const provided_datasets_chanel_informtaion = [
    {
      id: 'provided_datasets_chanel_informtaion',
      datasets: {
        name: "",
        description: "CHANEL datasets is Task 1 and Task 2 oriented. The source language is English."
      },
      info: [
        {id: 'dbdc_dataset',
        name_dataset: 'DBDC',
        spanish_translation: '✔',
        chinese_translation: '',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'cmu_dog_dataset',
        name_dataset: 'CMU_DoG',
        spanish_translation: '✔',
        chinese_translation: '',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'cornell_movie_dialogs_dataset',
        name_dataset: 'Cornell Movie-Dialogs',
        spanish_translation: '✔',
        chinese_translation: '',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'dailydialog_dataset',
        name_dataset: 'DailyDialog',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'decode_dataset',
        name_dataset: 'DECODE',
        spanish_translation: '✔',
        chinese_translation: '',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'emotionlines_dataset',
        name_dataset: 'EmotionLines',
        spanish_translation: '✔',
        chinese_translation: '',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'empathicdialogues_dataset',
        name_dataset: 'EmpathicDialogues',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'holl_e_dataset',
        name_dataset: 'Holl-E',
        spanish_translation: '✔',
        chinese_translation: '',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'meena_dataset',
        name_dataset: 'MEENA',
        spanish_translation: '✔',
        chinese_translation: '',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'meld_dataset',
        name_dataset: 'MELD',
        spanish_translation: '✔',
        chinese_translation: '',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'metalwoz_dataset',
        name_dataset: 'MetalWOz',
        spanish_translation: '✔',
        chinese_translation: '',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'movie_dic_dataset',
        name_dataset: 'Movie-DiC',
        spanish_translation: '✔',
        chinese_translation: '',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'personachat_dataset',
        name_dataset: 'PersonaChat',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'sentimentliar_dataset',
        name_dataset: 'SentimentLIAR',
        spanish_translation: '✔',
        chinese_translation: '',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'switchboard_coherence_dataset',
        name_dataset: 'Switchboard Coherence',
        spanish_translation: '✔',
        chinese_translation: '',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'topical_chat_dataset',
        name_dataset: 'Topical-Chat',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'wizard_of_wikipedia_dataset',
        name_dataset: 'Wizard of Wikipedia',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
        {id: 'wochat_dataset',
        name_dataset: 'WOCHAT',
        spanish_translation: '✔',
        chinese_translation: '',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '',
        annotation_granularity: 'Turn-level'},
      ]
    }
  ]

  const provided_datasets_dstc10_informtation = [
    {
      id: 'provided_datasets_dstc10_informtation',
      datasets: {
        name: "",
        description: "DSTC10 datasets is Task 1 and Task 2 oriented. The source language is English."
      },
      info: [
        {id: 'convai-grade_dataset',
        name_dataset: 'CONVAI2-GRADE (CG)',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '✔',
        annotation_granularity: 'Turn-level'},
        {id: 'dailydialog-grade_dataset',
        name_dataset: 'DAILYDIALOG-GRADE (DH)',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '✔',
        annotation_granularity: 'Turn-level'},
        {id: 'dailydialog-gupta_dataset',
        name_dataset: 'DAILYDIALOG-GUPTA (DG)',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '✔',
        annotation_granularity: 'Turn-level'},
        {id: 'dailydialog-zhao_dataset',
        name_dataset: 'DAILYDIALOG-ZHAO (DZ)',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '✔',
        annotation_granularity: 'Turn-level'},
        {id: 'dstc7_dataset',
        name_dataset: 'DSTC7 (D7)',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '✔',
        annotation_granularity: 'Turn-level'},
        {id: 'empathetic-grade_dataset',
        name_dataset: 'EMPATHETIC-GRADE (EG)',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '✔',
        annotation_granularity: 'Turn-level'},
        {id: 'fed-dial_dataset',
        name_dataset: 'FED-DIAL (FD)',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '✔',
        annotation_granularity: 'Dialogue-level'},
        {id: 'fed-turn_dataset',
        name_dataset: 'FED-TURN (FT)',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '✔',
        annotation_granularity: 'Turn-level'},
        {id: 'humod_dataset',
        name_dataset: 'HUMOD (HU)',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '✔',
        annotation_granularity: 'Turn-level'},
        {id: 'persona-see_dataset',
        name_dataset: 'PERSONA-SEE (PS)',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '✔',
        annotation_granularity: 'Dialogue-level'},
        {id: 'persona-usr_dataset',
        name_dataset: 'PERSONA-USR (PU)',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '✔',
        annotation_granularity: 'Turn-level'},
        {id: 'persona-zhao_dataset',
        name_dataset: 'PERSONA-ZHAO (PZ)',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '✔',
        annotation_granularity: 'Turn-level'},
        {id: 'topical-usr_dataset',
        name_dataset: 'TOPICAL-USR (TU)',
        spanish_translation: '✔',
        chinese_translation: '✔',
        english_translation: '',
        english_Back_translation: '✔',
        paraphrases: '✔',
        sentiment_analysis: '✔',
        content_moderate: '✔',
        human_annotations: '✔',
        annotation_granularity: 'Turn-level'},
      ]
    }
  ]

  const provided_datasets_cdial_information = [
    {
      id: 'provided_datasets_cdial_information',
      datasets: {
        name: "",
        description: "CDIAL dataset is Task 1 oriented. The source language is Chinese."
      },
      info: [
        {id: 'ecm_dataset',
        name_dataset: 'ECM',
        spanish_translation: '',
        chinese_translation: '',
        english_translation: '✔',
        english_Back_translation: '',
        paraphrases: '',
        sentiment_analysis: '',
        content_moderate: '',
        human_annotations: '✔'},
        {id: 'kdconv_dataset',
        name_dataset: 'KDCONV',
        spanish_translation: '',
        chinese_translation: '',
        english_translation: '✔',
        english_Back_translation: '',
        paraphrases: '',
        sentiment_analysis: '',
        content_moderate: '',
        human_annotations: '✔'},
        {id: 'lccc_dataset',
        name_dataset: 'LCCC',
        spanish_translation: '',
        chinese_translation: '',
        english_translation: '✔',
        english_Back_translation: '',
        paraphrases: '',
        sentiment_analysis: '',
        content_moderate: '',
        human_annotations: '✔'},
      ]
    }
  ]

  const provided_datasets_chanel_statistics_train = [
    {
      id: 'provided_datasets_chanel_statistics_train',
      datasets: {
        name: "",
        description: "Data sets that make up the train set."
      },
      info: [
        {id: 'dbdc_dataset',
        name: 'DBDC',
        turns: '8,509',
        dialogues: '415',
        average_turn_dial: '20.50',
        average_words_turn: '7.31',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'cmu_dog_dataset',
        name: 'CMU_DoG',
        turns: '95,305',
        dialogues: '4,221',
        average_turn_dial: '22.58',
        average_words_turn: '17.93',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'cornell_movie_dialogs_dataset',
        name: 'Cornell Movie-Dialogs',
        turns: '304,713',
        dialogues: '83,097',
        average_turn_dial: '3.67',
        average_words_turn: '13.72',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'dailydialog_dataset',
        name: 'DailyDialog',
        turns: '102,960',
        dialogues: '13,116',
        average_turn_dial: '7.85',
        average_words_turn: '13.96',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'decode_dataset',
        name: 'DECODE',
        turns: '296,105',
        dialogues: '35,426',
        average_turn_dial: '8.36',
        average_words_turn: '15.05',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'emotionlines_dataset',
        name: 'EmotionLines',
        turns: '14,503',
        dialogues: '1,000',
        average_turn_dial: '14.50',
        average_words_turn: '10.53',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'empathicdialogues_dataset',
        name: 'EmpathicDialogues',
        turns: '107,220',
        dialogues: '24,850',
        average_turn_dial: '4.31',
        average_words_turn: '15.88',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'holl_e_dataset',
        name: 'Holl-E',
        turns: '91,452',
        dialogues: '9,071',
        average_turn_dial: '10.08',
        average_words_turn: '17.74',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'meena_dataset',
        name: 'MEENA',
        turns: '3,675',
        dialogues: '193',
        average_turn_dial: '19.04',
        average_words_turn: '9.14',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'meld_dataset',
        name: 'MELD',
        turns: '23,197',
        dialogues: '1,592',
        average_turn_dial: '14.57',
        average_words_turn: '10.98',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'metalwoz_dataset',
        name: 'MetalWOz',
        turns: '432,036',
        dialogues: '37,884',
        average_turn_dial: '11.40',
        average_words_turn: '8.47',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'movie_dic_dataset',
        name: 'Movie-DiC',
        turns: '512,582',
        dialogues: '65,215',
        average_turn_dial: '7.86',
        average_words_turn: '13.82',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'personachat_dataset',
        name: 'PersonaChat',
        turns: '162,064',
        dialogues: '10,907',
        average_turn_dial: '14.86',
        average_words_turn: '11.72',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'sentimentliar_dataset',
        name: 'SentimentLIAR',
        turns: '12,781',
        dialogues: '12,781',
        average_turn_dial: '1.00',
        average_words_turn: '20.16',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'switchboard_coherence_dataset',
        name: 'Switchboard Coherence',
        turns: '12,059',
        dialogues: '1,000',
        average_turn_dial: '12.06',
        average_words_turn: '20.55',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'topical_chat_dataset',
        name: 'Topical-Chat',
        turns: '235,281',
        dialogues: '10,784',
        average_turn_dial: '21.82',
        average_words_turn: '23.23',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'wizard_of_wikipedia_dataset',
        name: 'Wizard of Wikipedia',
        turns: '201,999',
        dialogues: '22,311',
        average_turn_dial: '9.05',
        average_words_turn: '18.83',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'wochat_dataset',
        name: 'Wochat',
        turns: '19,881',
        dialogues: '607',
        average_turn_dial: '32.75',
        average_words_turn: '6.75',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'total_train',
        name: 'Total',
        turns: '2,636,322',
        dialogues: '334,470',
        average_turn_dial: '236.26',
        average_words_turn: '255.77',
        annotation_granularity: '',
        original_language: '',
        translation: ''},
      ]
    }
  ]

  const provided_datasets_chanel_statistics_dev = [
    {
      id: 'provided_datasets_chanel_statistics_dev',
      datasets: {
        name: "",
        description: "Data sets that make up the development set."
      },
      info: [
        {id: 'convai-grade_dataset',
        name: 'ConvAI2-GRADE',
        turns: '1,800',
        dialogues: '600',
        average_turn_dial: '3.00',
        average_words_turn: '12.07',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'dailydialog-grade_dataset',
        name: 'DailyDialog-GRADE',
        turns: '900',
        dialogues: '300',
        average_turn_dial: '3.00',
        average_words_turn: '12.60',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'dailydialog-gupta_dataset',
        name: 'DailyDialog-GUPTA',
        turns: '2,460',
        dialogues: '500',
        average_turn_dial: '4.92',
        average_words_turn: '12.37',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'dailydialog-zhao_dataset',
        name: 'DailyDialog-ZHAO',
        turns: '4,248',
        dialogues: '900',
        average_turn_dial: '4.72',
        average_words_turn: '12.41',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'dstc7_dataset',
        name: 'DSTC7',
        turns: '34,650',
        dialogues: '9,990',
        average_turn_dial: '3.47',
        average_words_turn: '15.39',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'empathetic-grade_dataset',
        name: 'Empathetic-GRADE',
        turns: '900',
        dialogues: '300',
        average_turn_dial: '3.00',
        average_words_turn: '16.65',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'fed-dial_dataset',
        name: 'FED-Dial',
        turns: '1,715',
        dialogues: '125',
        average_turn_dial: '13.72',
        average_words_turn: '11.10',
        annotation_granularity: 'Dial',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'fed-turn_dataset',
        name: 'FED-Turn',
        turns: '3,888',
        dialogues: '375',
        average_turn_dial: '10.37',
        average_words_turn: '10.78',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'humod_dataset',
        name: 'HUMOD',
        turns: '37,468',
        dialogues: '9,499',
        average_turn_dial: '3.94',
        average_words_turn: '7.97',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'persona-see_dataset',
        name: 'Persona-SEE',
        turns: '39,792',
        dialogues: '3,316',
        average_turn_dial: '12.00',
        average_words_turn: '9.00',
        annotation_granularity: 'Dial',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'persona-usr_dataset',
        name: 'PersonaChat-USR',
        turns: '2,790',
        dialogues: '300',
        average_turn_dial: '9.30',
        average_words_turn: '12.08',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'persona-zhao_dataset',
        name: 'PersonaChat-ZHAO',
        turns: '4,614',
        dialogues: '900',
        average_turn_dial: '5.13',
        average_words_turn: '12.06',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'topical-usr_dataset',
        name: 'TOPICAL-USR',
        turns: '4,032',
        dialogues: '360',
        average_turn_dial: '11.20',
        average_words_turn: '23.16',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'ecm_dataset',
        name: 'ECM-Eval',
        turns: '3,004',
        dialogues: '1,502',
        average_turn_dial: '2.00',
        average_words_turn: '13.13',
        annotation_granularity: 'Turn',
        original_language: 'Zh',
        translation: 'En'},
        {id: 'kdconv_dataset',
        name: 'KdConv-Eval',
        turns: '3,499',
        dialogues: '354',
        average_turn_dial: '9.88',
        average_words_turn: '21.11',
        annotation_granularity: 'Turn',
        original_language: 'Zh',
        translation: 'En'},
        {id: 'lccc_dataset',
        name: 'LCCC-Eval',
        turns: '3,009',
        dialogues: '589',
        average_turn_dial: '5.11',
        average_words_turn: '11.72',
        annotation_granularity: 'Turn',
        original_language: 'Zh',
        translation: 'En'},
        {id: 'total_dev',
        name: 'Total',
        turns: '148,769',
        dialogues: '29,910',
        average_turn_dial: '104.76',
        average_words_turn: '212.64',
        annotation_granularity: '',
        original_language: '',
        translation: ''},
      ]
    }
  ]

  const provided_datasets_chanel_statistics_test = [
    {
      id: 'provided_datasets_chanel_statistics_test',
      datasets: {
        name: "",
        description: "Data sets that make up the test set."
      },
      info: [
        {id: 'blenderbot3_dataset',
        name: 'BlenderBot3',
        turns: '679',
        dialogues: '21',
        average_turn_dial: '32.33',
        average_words_turn: '16.96',
        annotation_granularity: 'Turn/Dial',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'chatgpt_dataset',
        name: 'ChatGPT',
        turns: '462',
        dialogues: '21',
        average_turn_dial: '22.00',
        average_words_turn: '91.07',
        annotation_granularity: 'Turn/Dial',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'gpt-3.5_dataset',
        name: 'GPT-3.5',
        turns: '560',
        dialogues: '17',
        average_turn_dial: '32.94',
        average_words_turn: '23.73',
        annotation_granularity: 'Turn/Dial',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'hcchinese_dataset',
        name: 'HCChinese',
        turns: '2,017',
        dialogues: '187',
        average_turn_dial: '10.79',
        average_words_turn: '8.08',
        annotation_granularity: 'Turn/Dial',
        original_language: 'Zh',
        translation: 'En'},
        {id: 'chateval_dataset',
        name: 'ChatEval',
        turns: '400',
        dialogues: '200',
        average_turn_dial: '2.00',
        average_words_turn: '8.13',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'dstc10_dataset',
        name: 'DSTC10',
        turns: '112',
        dialogues: '28',
        average_turn_dial: '4.00',
        average_words_turn: '14.00',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'jsalt_dataset',
        name: 'JSALT',
        turns: '46',
        dialogues: '13',
        average_turn_dial: '3.54',
        average_words_turn: '17.26',
        annotation_granularity: 'Turn',
        original_language: 'En',
        translation: 'Zh/Es'},
        {id: 'total_test',
        name: 'Total',
        turns: '4,276',
        dialogues: '487',
        average_turn_dial: '107.60',
        average_words_turn: '179.23',
        annotation_granularity: '',
        original_language: '',
        translation: ''},
      ]
    }
  ]

  const multilingual_dev = [
    {
      id: 'multilingual_dev',
      evaldev: { 
        name: "Task 1:  Multilingual Metrics (development)",
        description: ""
      },
      results: [
        {
          id: 'en_result',
          sys: 'AM-FM EN',
          cg: '0.3373',
          dh: '0.0916',
          dg: '0.2811',
          dz: '0.1433',
          d7: '0.2469',
          eg: '0.2548',
          fd: '0.1269',
          ft: '0.0264',
          hm: '0.1258',
          ps: '0.0262',
          pu: '0.0823',
          pz: '0.4489',
          tu: '0.1149',
          avg: '0.1774'
        },
        {
          id: 'es_result',
          sys: 'AM-FM ES',
          cg: '0.3094',
          dh: '0.1053',
          dg: '0.2146',
          dz: '0.1170',
          d7: '0.2317',
          eg: '0.2001',
          fd: '0.1172',
          ft: '-0.0120',
          hm: '0.1019',
          ps: '0.0236',
          pu: '0.0634',
          pz: '0.4118',
          tu: '0.1086',
          avg: '0.1551'
        },
        {
          id: 'zh_result',
          sys: 'AM-FM ZH',
          cg: '0.2989',
          dh: '0.0873',
          dg: '0.2382',
          dz: '0.1391',
          d7: '0.2206',
          eg: '0.2115',
          fd: '0.0819',
          ft: '-0.0254',
          hm: '0.0990',
          ps: '0.0198',
          pu: '0.0849',
          pz: '0.3821',
          tu: '0.0849',
          avg: '0.1518'
        },
      ]
    }
  ]

  const robust_dev = [
    {
      id: 'robust_dev',
      evaldev: { 
        name: "Task 2: Robust Metrics (development)",
        description: ""
      },
      results: [
        {
          id: 'pa_result',
          sys: 'AM-FM PAR',
          cg: '0.2842',
          dh: '0.0512',
          dg: '0.2879',
          dz: '0.1356',
          d7: '0.0374',
          eg: '0.2452',
          fd: '0.1243',
          ft: '-0.0039',
          hm: '0.1080',
          ps: '0.0192',
          pu: '0.0730',
          pz: '0.4241',
          tu: '0.0872',
          avg: '0.1447'
        },
      ]
    }
  ]

  const multilingual_test_turn = [
    {
      id: 'multilingual_test_turn',
      evaltest: { 
        name: "Task 1:  Multilingual Metrics Turn-Level (test)",
        description: ""
      },
      results: [
        {
          id: 'baseline_t1t_result',
          sys: 'baseline_t1t',
          en: '0.2940',
          zh: '0.0753',
          es: '0.1826',
          lan_avg: '0.1840',
          rank_sub: '11',
          rank_team: '4'
        },
        {
          id: 'team2_t1t_s1_result',
          sys: 'team2_t1t_s1',
          en: '0.1469',
          zh: '0.1054',
          es: '0.0808',
          lan_avg: '0.1110',
          rank_sub: '12',
          rank_team: '5'
        },
        {
          id: 'team4_t1t_s1_result',
          sys: 'team4_t1t_s1',
          en: '0.4818',
          zh: '0.3936',
          es: '0.5890',
          lan_avg: '0.4881',
          rank_sub: '1',
          rank_team: '1'
        },
        {
          id: 'team4_t1t_s2_result',
          sys: 'team4_t1t_s2',
          en: '0.2625',
          zh: '0.3096',
          es: '0.5056',
          lan_avg: '0.3592',
          rank_sub: '6',
          rank_team: ''
        },
        {
          id: 'team4_t1t_s3_result',
          sys: 'team4_t1t_s3',
          en: '0.4795',
          zh: '0.3656',
          es: '0.5409',
          lan_avg: '0.4620',
          rank_sub: '2',
          rank_team: ''
        },
        {
          id: 'team4_t1t_s4_result',
          sys: 'team4_t1t_s4',
          en: '0.4586',
          zh: '0.3618',
          es: '0.5412',
          lan_avg: '0.4539',
          rank_sub: '3',
          rank_team: ''
        },
        {
          id: 'team5_t1t_s1_result',
          sys: 'team5_t1t_s1',
          en: '0.3702',
          zh: '0.0701',
          es: '0.1983',
          lan_avg: '0.2129',
          rank_sub: '9',
          rank_team: '3'
        },
        {
          id: 'team5_t1t_s2_result',
          sys: 'team5_t1t_s2',
          en: '0.2690',
          zh: '0.1375',
          es: '0.2281',
          lan_avg: '0.2116',
          rank_sub: '10',
          rank_team: ''
        },
        {
          id: 'team7_t1t_s1_result',
          sys: 'team7_t1t_s1',
          en: '0.1275',
          zh: '0.2557',
          es: '0.4753',
          lan_avg: '0.2862',
          rank_sub: '7',
          rank_team: ''
        },
        {
          id: 'team7_t1t_s2_result',
          sys: 'team7_t1t_s2',
          en: '0.2314',
          zh: '0.3163',
          es: '0.5478',
          lan_avg: '0.3652',
          rank_sub: '5',
          rank_team: ''
        },
        {
          id: 'team7_t1t_s3_result',
          sys: 'team7_t1t_s3',
          en: '0.1083',
          zh: '0.2480',
          es: '0.4799',
          lan_avg: '0.2787',
          rank_sub: '8',
          rank_team: ''
        },
        {
          id: 'team7_t1t_s4_result',
          sys: 'team7_t1t_s4',
          en: '0.2214',
          zh: '0.3112',
          es: '0.5644',
          lan_avg: '0.3657',
          rank_sub: '4',
          rank_team: '2'
        }
      ]
    }
  ]

  const multilingual_test_dial = [
    {
      id: 'multilingual_test_dial',
      evaltest: { 
        name: "Task 1:  Multilingual Metrics Dialogue-Level (test)",
        description: ""
      },
      results: [
        {
          id: 'baseline_t1d_result',
          sys: 'baseline_t1d',
          en: '0.2414',
          zh: '0.4648',
          es: '0.8080',
          lan_avg: '0.5047',
          rank_sub: '4',
          rank_team: '2'
        },
        {
          id: 'team4_t1d_s1_result',
          sys: 'team4_t1d_s1',
          en: '0.5342',
          zh: '0.7133',
          es: '0.8080',
          lan_avg: '0.6852',
          rank_sub: '1',
          rank_team: '1'
        },
        {
          id: 'team4_t1d_s2_result',
          sys: 'team4_t1d_s2',
          en: '0.3295',
          zh: '0.7030',
          es: '0.2500',
          lan_avg: '0.4275',
          rank_sub: '5',
          rank_team: ''
        },
        {
          id: 'team4_t1d_s3_result',
          sys: 'team4_t1d_s3',
          en: '0.5251',
          zh: '0.6701',
          es: '0.8080',
          lan_avg: '0.6677',
          rank_sub: '2',
          rank_team: ''
        },
        {
          id: 'team4_t1d_s4_result',
          sys: 'team4_t1d_s4',
          en: '0.5039',
          zh: '0.5859',
          es: '0.5915',
          lan_avg: '0.5604',
          rank_sub: '3',
          rank_team: ''
        },
        {
          id: 'team5_t1d_s1_result',
          sys: 'team5_t1d_s1',
          en: '0.1865',
          zh: '0.1356',
          es: '0.6830',
          lan_avg: '0.3350',
          rank_sub: '6',
          rank_team: '3'
        } 
      ]
    }
  ]
  
  const robust_test_turn = [
    {
      id: 'robust_test_turn',
      evaltest: { 
        name: "Task 2: Robust Metrics Turn-Level (test)",
        description: ""
      },
      results: [
        {
          id: 'baseline_t2t',
          sys: 'baseline_t2t',
          ro_avg: '0.3387',
          rank_sub: '7',
          rank_team: '4'
        },
        {
          id: 'team1_t2t_s1',
          sys: 'team1_t2t_s1',
          ro_avg: '0.1537',
          rank_sub: '11',
          rank_team: '6'
        },
        {
          id: 'team3_t2t_s1',
          sys: 'team3_t2t_s1',
          ro_avg: '0.1306',
          rank_sub: '13',
          rank_team: ''
        },
        {
          id: 'team3_t2t_s2',
          sys: 'team3_t2t_s2',
          ro_avg: '0.1277',
          rank_sub: '14',
          rank_team: ''
        },
        {
          id: 'team3_t2t_s3',
          sys: 'team3_t2t_s3',
          ro_avg: '0.1469',
          rank_sub: '12',
          rank_team: ''
        },
        {
          id: 'team3_t2t_s4',
          sys: 'team3_t2t_s4',
          ro_avg: '0.2697',
          rank_sub: '9',
          rank_team: '5'
        },
        {
          id: 'team4_t2t_s1',
          sys: 'team4_t2t_s1',
          ro_avg: '0.4890',
          rank_sub: '1',
          rank_team: '1'
        },
        {
          id: 'team4_t2t_s2',
          sys: 'team4_t2t_s2',
          ro_avg: '0.3320',
          rank_sub: '8',
          rank_team: ''
        },
        {
          id: 'team4_t2t_s3',
          sys: 'team4_t2t_s3',
          ro_avg: '0.4756',
          rank_sub: '2',
          rank_team: ''
        },
        {
          id: 'team4_t2t_s4',
          sys: 'team4_t2t_s4',
          ro_avg: '0.4427',
          rank_sub: '3',
          rank_team: ''
        },
        {
          id: 'team6_t2t_s1',
          sys: 'team6_t2t_s1',
          ro_avg: '0.4190',
          rank_sub: '4',
          rank_team: '2'
        },
        {
          id: 'team6_t2t_s2',
          sys: 'team6_t2t_s2',
          ro_avg: '0.1742',
          rank_sub: '10',
          rank_team: ''
        },
        {
          id: 'team6_t2t_s3',
          sys: 'team6_t2t_s3',
          ro_avg: '0.0807',
          rank_sub: '15',
          rank_team: ''
        },
        {
          id: 'team7_t2t_s1',
          sys: 'team7_t2t_s1',
          ro_avg: '0.3833',
          rank_sub: '5',
          rank_team: '3'
        },
        {
          id: 'team7_t2t_s2',
          sys: 'team7_t2t_s2',
          ro_avg: '0.3643',
          rank_sub: '6',
          rank_team: ''
        }
      ]
    }
  ]

  const robust_test_dial = [
    {
      id: 'robust_test_dial',
      evaltest: { 
        name: "Task 2: Robust Metrics Dialogue-Level (test)",
        description: ""
      },
      results: [
        {
          id: 'baseline_t2d_result',
          sys: 'baseline_t2d',
          ro_avg: '0.4800',
          rank_sub: '1',
          rank_team: '1'
        },
        {
          id: 'team1_t2d_s1_result',
          sys: 'team1_t2d_s1',
          ro_avg: '0.1111',
          rank_sub: '8',
          rank_team: '4'
        },
        {
          id: 'team3_t2d_s1_result',
          sys: 'team3_t2d_s1',
          ro_avg: '0.2196',
          rank_sub: '6',
          rank_team: '3'
        },
        {
          id: 'team3_t2d_s2_result',
          sys: 'team3_t2d_s2',
          ro_avg: '0.1453',
          rank_sub: '7',
          rank_team: ''
        },
        {
          id: 'team4_t2d_s1_result',
          sys: 'team4_t2d_s1',
          ro_avg: '0.3031',
          rank_sub: '2',
          rank_team: '2'
        },
        {
          id: 'team4_t2d_s2_result',
          sys: 'team4_t2d_s2',
          ro_avg: '0.2335',
          rank_sub: '5',
          rank_team: ''
        },
        {
          id: 'team4_t2d_s3_result',
          sys: 'team4_t2d_s3',
          ro_avg: '0.2979',
          rank_sub: '3',
          rank_team: ''
        },
        {
          id: 'team4_t2d_s4_result',
          sys: 'team4_t2d_s4',
          ro_avg: '0.2492',
          rank_sub: '4',
          rank_team: ''
        }
      ]
    }
  ]

  return { provided_datasets_summary, provided_datasets_chanel_informtaion, provided_datasets_dstc10_informtation, provided_datasets_cdial_information, provided_datasets_chanel_statistics_train, provided_datasets_chanel_statistics_dev, provided_datasets_chanel_statistics_test, multilingual_dev, robust_dev, multilingual_test_turn, multilingual_test_dial, robust_test_turn, robust_test_dial }

};

export default DSTC12;
